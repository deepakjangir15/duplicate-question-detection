{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2ie9qM3wM_g",
        "outputId": "47148cac-30af-420c-fe46-287fca319650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit==0.86.0\n",
            "  Downloading streamlit-0.86.0-py2.py3-none-any.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (1.3.5)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (1.21.6)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (21.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (2.8.2)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (0.8.0)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (3.19.6)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (2.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (2.25.1)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (7.1.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (3.1.30)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (0.10.2)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (0.8.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (9.0.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (1.5.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (0.20.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (5.2.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (22.2.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==0.86.0) (6.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==0.86.0) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==0.86.0) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==0.86.0) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==0.86.0) (4.3.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython!=3.1.19->streamlit==0.86.0) (4.0.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.21.0->streamlit==0.86.0) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->streamlit==0.86.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->streamlit==0.86.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->streamlit==0.86.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->streamlit==0.86.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->streamlit==0.86.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->streamlit==0.86.0) (4.0.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators->streamlit==0.86.0) (4.4.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==0.86.0) (5.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=3.2.0->streamlit==0.86.0) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.86.0) (5.10.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.86.0) (0.19.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit==0.86.0) (3.11.0)\n",
            "Installing collected packages: base58, streamlit\n",
            "  Attempting uninstall: streamlit\n",
            "    Found existing installation: streamlit 1.9.0\n",
            "    Uninstalling streamlit-1.9.0:\n",
            "      Successfully uninstalled streamlit-1.9.0\n",
            "Successfully installed base58-2.1.1 streamlit-0.86.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.8/dist-packages (5.2.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ],
      "source": [
        "# Install two libraries used to create a GUI and host it online on colab\n",
        "!pip install streamlit==0.86.0\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "!ngrok authtoken 261WeU8cQS95u1eoGredriNItwn_2ZsF6FKumDXm8H1geNLeo\n",
        "\n",
        "!streamlit run --server.port 80 app.py &>/dev/null&\n",
        "\n",
        "publ_url = ngrok.connect(port='8501')\n",
        "\n",
        "publ_url "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MORodDDfPD_n",
        "outputId": "a7bc7ca3-463c-4edf-8bec-d9d3b0a4daa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://b72f-130-211-112-27.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRpky4YC92ac",
        "outputId": "494d840b-d76d-45e9-a832-adb5a57bf65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import re\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "PAGE_CONFIG = {\"page_title\":\"Project Dissertation - KU\",\"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n",
        "\n",
        "my_details = pd.DataFrame({\"Name\": ['Deepak Jangir Dayanand'],\"KU ID\": ['K2161089']})\n",
        "my_details.set_index('Name', inplace=True)\n",
        "\n",
        "\n",
        "## Importing all the models implemented\n",
        "model_rf = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Deepak Dissertation/saved_models/rf_model_v2.pkl', 'rb'))\n",
        "model_cv = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Deepak Dissertation/saved_models/cv_model.pkl', 'rb'))\n",
        "\n",
        "def lemmatize_clean_text(text):\n",
        "\n",
        "    # Lemmatize words\n",
        "  def get_pos_tag(tag):\n",
        "      if tag.startswith('J'):\n",
        "          return wordnet.ADJ\n",
        "      elif tag.startswith('V'):\n",
        "          return wordnet.VERB\n",
        "      elif tag.startswith('N'):\n",
        "          return wordnet.NOUN\n",
        "      elif tag.startswith('R'):\n",
        "          return wordnet.ADV\n",
        "      else:\n",
        "          # Default lemmatization\n",
        "          return wordnet.NOUN\n",
        "\n",
        "  regex = [\n",
        "      r'<[^>]+>', #HTML tags\n",
        "      r'@(\\w+)', # @-mentions\n",
        "      r\"#(\\w+)\", # hashtags\n",
        "      r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        "      r'[^0-9a-z #+_\\\\r\\\\n\\\\t]', #BAD SYMBOLS\n",
        "  ]\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "  REPLACE_URLS = re.compile(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+')\n",
        "  REPLACE_HASH = re.compile(r'#(\\w+)')\n",
        "  REPLACE_AT = re.compile(r'@(\\w+)')\n",
        "  REPLACE_HTML_TAGS = re.compile(r'<[^>]+>')\n",
        "  REPLACE_BY = re.compile(r\"[^a-z0-9\\-]\")\n",
        "\n",
        "  STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "  text = text.lower()\n",
        "  text = REPLACE_HTML_TAGS.sub(' ',text)\n",
        "  text = REPLACE_URLS.sub('', text)\n",
        "  text = REPLACE_HASH.sub('', text)\n",
        "  text = REPLACE_AT.sub('', text)\n",
        "  text = REPLACE_BY.sub(' ', text)\n",
        "\n",
        "  text = \" \".join(lemmatizer.lemmatize(word.strip(), get_pos_tag(pos_tag([word.strip()])[0][1])) \\\n",
        "                  for word in text.split() if word not in STOP_WORDS and len(word)>3)\n",
        "\n",
        "  return text\n",
        "\n",
        "def convert_and_combine(q1,q2):\n",
        "\n",
        "\tquestions = [str(q1)] + [str(q2)]\n",
        "\n",
        "\tq1_arr, q2_arr = np.vsplit(model_cv.transform(questions).toarray(),2)\n",
        "\ttemp_df1 = pd.DataFrame(q1_arr)\n",
        "\ttemp_df2 = pd.DataFrame(q2_arr)\n",
        "\n",
        "\ttemp_df = pd.concat([temp_df1, temp_df2], axis=1)\n",
        "\n",
        "\treturn temp_df\n",
        "\n",
        "def extract_features(q1,q2):\n",
        "\n",
        "\td = {}\n",
        "\n",
        "\tdef fetch_common_words(q1,q2):\n",
        "\t\tw1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n",
        "\t\tw2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))\n",
        "\t\treturn len(w1 & w2)\n",
        "\n",
        "\tdef total_words(q1,q2):\n",
        "\t\tw1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n",
        "\t\tw2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n",
        "\t\treturn (len(w1) + len(w2))\n",
        "\n",
        "\n",
        "\td['q1len'] = len(q1)\n",
        "\td['q2len'] = len(q2)\n",
        "\td['q1_no_words'] = len(q1.split(\" \"))\n",
        "\td['q2_no_words'] = len(q2.split(\" \"))\n",
        "\td['common_words'] = fetch_common_words(q1,q2)\n",
        "\td['total_words'] = total_words(q1,q2)\n",
        "\td['shared_words'] = round(d['common_words']/d['total_words'],2)\n",
        "\n",
        "\treturn pd.DataFrame([d])\n",
        " \n",
        "\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "def main():\n",
        "\tst.title(\"CQA duplicate question detection using Random Forest Model\")\n",
        "\twith st.sidebar:\n",
        "\t\tst.subheader(\"More about the Model\")\n",
        "\t\tmenu = [\"Home\",\"About\"]\n",
        "\n",
        "\t\tchoice = st.sidebar.selectbox(label = \"Choose an option\",options=menu)\n",
        "\t\tst.sidebar.table(my_details)\n",
        "\n",
        "\tst.subheader(\"Test if your set of questions are duplicate or not!!\")\n",
        "\t\n",
        "\n",
        "\tif choice == 'Home':\n",
        "\t\tq1 = st.text_input('Input your question 1 here:') \t\n",
        "\t\tq2 = st.text_input('Input your question 2 here:')\n",
        "\t\n",
        "\t\tq1 = lemmatize_clean_text(q1)\n",
        "\t\tq2 = lemmatize_clean_text(q2)\n",
        "\n",
        "\t\tif st.button(label='Check Duplication',help='Submit the questions'):\n",
        "\t\t\t\t\n",
        "\t\t\t\tconverted = convert_and_combine(q1,q2)\n",
        "\t\n",
        "\t\t\t\tcombined_data_features = pd.concat([extract_features(str(q1),str(q2)), converted], axis=1)\n",
        "\t\t\n",
        "\t\t\t\tvalue = model_rf.predict([combined_data_features.iloc[0].values])\n",
        "\n",
        "\t\t\t\tif value[0] == 0:\n",
        "\t\t\t\t\tst.success(\"The two questions are not Duplicate\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tst.error('The two questions are duplicate')\n",
        "\n",
        "\telif choice == 'About':\n",
        "\t\tst.markdown(\n",
        "\t\t\t\t\t\t\t\t\"\"\"\n",
        "\t\t\t\t\t\t\t\tThe reason why the Random Forest Classifier was chosen is that it performed the best on the validation dataset in comparison to the other models.\n",
        "\t\t\t\t\t\t\t\t- It first pre-processes individual questions posted here to eliminate all the bad symbols and lemmatizes the words.\n",
        "\t\t\t\t\t\t\t\t- Next the preprocessed questions are converted to 3000 numerical features each and upon these features, feature engineering is applied and more new features are dervied.\n",
        "\t\t\t\t\t\t\t\t- Post everything, the data is passed to the Random Forest model and the outputs are predicted and displayed on the screen. \n",
        "\t\t\t\t\t\t\t\t\"\"\"\n",
        "\t\t\t\t\t\t\t\t)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBHAqfQY-PsY",
        "outputId": "6e573d4a-c4ab-4b11-e0aa-eeebe0b31db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root       15652   15525  0 04:39 ?        00:00:00 /bin/bash -c ps -eaf | grep streamlit\n",
            "root       15654   15652  0 04:39 ?        00:00:00 grep streamlit\n"
          ]
        }
      ],
      "source": [
        "!ps -eaf | grep streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWu9RkM7-U6w",
        "outputId": "51fae8a4-7088-4caa-eb63-f334acc9e542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: kill: (15415) - No such process\n"
          ]
        }
      ],
      "source": [
        "!kill 15415"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPBivcA91kT0"
      },
      "outputs": [],
      "source": [
        "!killall ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_97MYRmy1ohK"
      },
      "outputs": [],
      "source": [
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL5uEsOR93r5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}